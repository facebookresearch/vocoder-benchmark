dataset:
  batch_size: 16
  frames_per_clip: 32
  clips_per_utterance: 10
  padding_frames: 0
  dataloader_num_workers: 4
  dataloader_prefetch_factor: 4
model:
  # Generator Config
  generator_type: "MelGANGenerator"
  generator_params:
    in_channels: 80               # Number of input channels.
    out_channels: 1               # Number of output channels.
    kernel_size: 7                # Kernel size of initial and final conv layers.
    channels: 512                 # Initial number of channels for conv layers.
    upsample_scales: [4, 5, 3, 5] # List of Upsampling scales.
    stack_kernel_size: 3          # Kernel size of dilated conv layers in residual stack.
    stacks: 3                     # Number of stacks in a single residual stack module.
    use_weight_norm: True         # Whether to use weight normalization.
    use_causal_conv: False        # Whether to use causal convolution.
  generator_optimizer_type: "RAdam"
  generator_grad_norm: 10         # Generator's gradient norm.
  generator_optimizer:
    lr: 0.0001             # Generator's learning rate.
    eps: 1.0e-6            # Generator's epsilon.
    weight_decay: 0.0      # Generator's weight decay coefficient.
  generator_scheduler_type: "StepLR" # Generator's scheduler type.
  generator_scheduler_params:
    step_size: 4000000     # Generator's scheduler step size.
    gamma: 0.5                          # Generator's scheduler gamma.

  # Discriminator Config
  discriminator_type: "MelGANMultiScaleDiscriminator"
  discriminator_params:
    in_channels: 1                    # Number of input channels.
    out_channels: 1                   # Number of output channels.
    scales: 3                         # Number of multi-scales.
    downsample_pooling: "AvgPool1d"   # Pooling type for the input downsampling.
    downsample_pooling_params:        # Parameters of the above pooling function.
        kernel_size: 4
        stride: 2
        padding: 1
        count_include_pad: False
    kernel_sizes: [5, 3]              # List of kernel size.
    channels: 16                      # Number of channels of the initial conv layer.
    max_downsample_channels: 1024     # Maximum number of channels of downsampling layers.
    downsample_scales: [4, 4, 4, 4]   # List of downsampling scales.
    nonlinear_activation: "LeakyReLU" # Nonlinear activation function.
    nonlinear_activation_params:      # Parameters of nonlinear activation function.
        negative_slope: 0.2
    use_weight_norm: True             # Whether to use weight norm.
  discriminator_optimizer_type: "RAdam"
  discriminator_grad_norm: 1        # Discriminator's gradient norm.
  discriminator_optimizer:
    lr: 0.00005            # Discriminator's learning rate.
    eps: 1.0e-6            # Discriminator's epsilon.
    weight_decay: 0.0      # Discriminator's weight decay coefficient.
  discriminator_scheduler_type: "StepLR" # Discriminator's scheduler type.
  discriminator_scheduler_params:
    step_size: 4000000     # Generator's scheduler step size.
    gamma: 0.5                              # Discriminator's scheduler gamma.

  # Loss
  stft_loss_params:
      fft_sizes: [1024, 2048, 512]  # List of FFT size for STFT-based loss.
      hop_sizes: [120, 240, 50]     # List of hop size for STFT-based loss
      win_lengths: [600, 1200, 240] # List of window length for STFT-based loss.
      window: "hann_window"         # Window function for STFT-based loss
  subband_stft_loss_params:
      fft_sizes: [384, 683, 171]  # List of FFT size for STFT-based loss.
      hop_sizes: [30, 60, 10]     # List of hop size for STFT-based loss
      win_lengths: [150, 300, 60] # List of window length for STFT-based loss.
      window: "hann_window"       # Window function for STFT-based loss
  use_feat_match_loss: true # Whether to use feature matching loss.
  lambda_feat_match: 25.5
  lambda_adv: 4.0          # Loss balancing coefficient for adversarial loss.

  # Training
  n_iterations: 4000000
  discriminator_train_start_steps: 100000
